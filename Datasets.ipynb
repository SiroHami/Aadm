{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMcX4iojhJ+fFJdDdthoUw3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_pdXE8FxkoJ","executionInfo":{"status":"ok","timestamp":1684411960511,"user_tz":-540,"elapsed":2843,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}},"outputId":"3f52d042-7fe9-47ef-9107-c243613b6f78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJRBn9mrxtlT","executionInfo":{"status":"ok","timestamp":1684411974727,"user_tz":-540,"elapsed":632,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}},"outputId":"56c65a8e-d81b-4722-b6e8-0c5228143576"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Datasets\n"]}]},{"cell_type":"code","source":["!git clone https://Bayan-K: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@github.com/Bayan-K/Aadm.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5T5MvIjhx5h2","executionInfo":{"status":"ok","timestamp":1684412146600,"user_tz":-540,"elapsed":1141,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}},"outputId":"6d472f5a-bc3f-4951-8ba3-797cc4f88ee0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Aadm'...\n","remote: Enumerating objects: 7, done.\u001b[K\n","remote: Counting objects: 100% (7/7), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 7 (delta 0), reused 4 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (7/7), 2.17 KiB | 47.00 KiB/s, done.\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Datasets/Aadm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v99udzt7ycA-","executionInfo":{"status":"ok","timestamp":1684412461686,"user_tz":-540,"elapsed":3,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}},"outputId":"a71af512-1068-4913-ca89-baa28208fd2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Datasets/Aadm\n"]}]},{"cell_type":"code","source":["!git config --global user.email 'nanyeon99@gmail.com'\n","!git config --global user.name 'Bayan-K'"],"metadata":{"id":"SOIhXDjDynHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add Datasets.ipynb"],"metadata":{"id":"XI_CQsWpy6L7","executionInfo":{"status":"ok","timestamp":1684420448635,"user_tz":-540,"elapsed":606,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"59944ebb-38cf-4fb1-f874-847b874fbfc0"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: pathspec 'Datasets.ipynb' did not match any files\n"]}]},{"cell_type":"code","source":["!git commit -m \"Datasets.ipynb\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CjkU8YLzqBx","executionInfo":{"status":"ok","timestamp":1684420462011,"user_tz":-540,"elapsed":587,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}},"outputId":"ca46de91-b40d-438a-df84-56f0a749446e"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","nothing to commit, working tree clean\n"]}]},{"cell_type":"code","source":["!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSzx1nKvzuax","executionInfo":{"status":"ok","timestamp":1684420466017,"user_tz":-540,"elapsed":1232,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}},"outputId":"7223f4c8-6557-4b00-a639-cd9a57ea1310"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Everything up-to-date\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","import numpy as np"],"metadata":{"id":"v-uyOJme1Gl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#All Dataset import\n","#Set Dataset Supervised, Semisupervised, Unsupervised\n","#Train, Test, Val "],"metadata":{"id":"32MbeLdl5PwU","executionInfo":{"status":"ok","timestamp":1684413953394,"user_tz":-540,"elapsed":3,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","  def __init__(self, data, labels=None, model=None,):\n","    self.data = data\n","    self.labels = labels\n","    self.model = model\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self):\n","    if self.labels is not None:\n","      #return \n","\n"],"metadata":{"id":"fDR_K3JnOHqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, data, labels=None, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        if self.labels is not None:\n","            return self.transform(self.data[index]), self.labels[index]\n","        else:\n","            return self.transform(self.data[index])\n","\n","\n","def calculate_mean_std():\n","    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n","\n","    imgs = [item[0] for item in trainset]  # item[0] and item[1] are image and its label\n","    imgs = torch.stack(imgs, dim=0).numpy()\n","\n","    # calculate mean over each channel (r,g,b)\n","    mean_r = imgs[:, 0, :, :].mean()\n","    mean_g = imgs[:, 1, :, :].mean()\n","    mean_b = imgs[:, 2, :, :].mean()\n","\n","    # calculate std over each channel (r,g,b)\n","    std_r = imgs[:, 0, :, :].std()\n","    std_g = imgs[:, 1, :, :].std()\n","    std_b = imgs[:, 2, :, :].std()\n","\n","    return mean_r, mean_g, mean_b, std_r, std_g, std_b\n","\n","\n","def import_datasets(data_dir, unsupervised_label_num= 250, batch_size=64):\n","    # Define data transformations\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    # Import the dataset\n","    full_dataset = datasets.ImageFolder(data_dir, transform=transform)\n","\n","    # Split the dataset into train, test, validation sets\n","    dataset_size = len(full_dataset)\n","    unsupervised_size = int(unsupervised_label_num * dataset_size)\n","    supervised_size = int((dataset_size - unsupervised_label_num) * dataset_size)\n","    train_size = dataset_size - unsupervised_size\n","    test_size = int(0.1 * train_size)  # 10% for testing\n","    valid_size = int(0.1 * train_size)  # 10% for validation\n","    train_size -= test_size + valid_size\n","\n","    unsupervised_data, _ = torch.utils.data.random_split(full_dataset, [unsupervised_size, dataset_size - unsupervised_size])\n","    supervised_data, _ = torch.utils.data.random_split(full_dataset, [supervised_size, dataset_size - supervised_size])\n","    train_data, test_data, valid_data = torch.utils.data.random_split(supervised_data, [train_size, test_size, valid_size])\n","\n","    # Create dataloaders\n","    unsupervised_loader = DataLoader(unsupervised_data, batch_size=batch_size, shuffle=True)\n","    supervised_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n","    valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n","\n","    return unsupervised_loader, supervised_loader, test_loader, valid_loader\n"],"metadata":{"id":"TcOlzqdT63Eo","executionInfo":{"status":"ok","timestamp":1684418915437,"user_tz":-540,"elapsed":496,"user":{"displayName":"yeon Nan","userId":"08796880923838814464"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","    CIFAR-10 classification dataset.\n","\"\"\"\n","\n","import os\n","from torchvision.datasets import CIFAR10\n","import torchvision.transforms as transforms\n","from .dataset_metainfo import DatasetMetaInfo\n","\n","\n","class CIFAR10Fine(CIFAR10):\n","    \"\"\"\n","    CIFAR-10 image classification dataset.\n","\n","\n","    Parameters:\n","    ----------\n","    root : str, default '~/.torch/datasets/cifar10'\n","        Path to temp folder for storing data.\n","    mode : str, default 'train'\n","        'train', 'val', or 'test'.\n","    transform : function, default None\n","        A function that takes data and label and transforms them.\n","    \"\"\"\n","    def __init__(self,\n","                 root=os.path.join(\"~\", \".torch\", \"datasets\", \"cifar10\"),\n","                 mode=\"train\",\n","                 transform=None):\n","        super(CIFAR10Fine, self).__init__(\n","            root=root,\n","            train=(mode == \"train\"),\n","            transform=transform,\n","            download=True)\n","\n","\n","class CIFAR10MetaInfo(DatasetMetaInfo):\n","    def __init__(self):\n","        super(CIFAR10MetaInfo, self).__init__()\n","        self.label = \"CIFAR10\"\n","        self.short_label = \"cifar\"\n","        self.root_dir_name = \"cifar10\"\n","        self.dataset_class = CIFAR10Fine\n","        self.num_training_samples = 50000\n","        self.in_channels = 3\n","        self.num_classes = 10\n","        self.input_image_size = (32, 32)\n","        self.train_metric_capts = [\"Train.Err\"]\n","        self.train_metric_names = [\"Top1Error\"]\n","        self.train_metric_extra_kwargs = [{\"name\": \"err\"}]\n","        self.val_metric_capts = [\"Val.Err\"]\n","        self.val_metric_names = [\"Top1Error\"]\n","        self.val_metric_extra_kwargs = [{\"name\": \"err\"}]\n","        self.saver_acc_ind = 0\n","        self.train_transform = cifar10_train_transform\n","        self.val_transform = cifar10_val_transform\n","        self.test_transform = cifar10_val_transform\n","        self.ml_type = \"imgcls\"\n","\n","\n","def cifar10_train_transform(ds_metainfo,\n","                            mean_rgb=(0.4914, 0.4822, 0.4465),\n","                            std_rgb=(0.2023, 0.1994, 0.2010),\n","                            jitter_param=0.4):\n","    assert (ds_metainfo is not None)\n","    assert (ds_metainfo.input_image_size[0] == 32)\n","    return transforms.Compose([\n","        transforms.RandomCrop(\n","            size=32,\n","            padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(\n","            brightness=jitter_param,\n","            contrast=jitter_param,\n","            saturation=jitter_param),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=mean_rgb,\n","            std=std_rgb)\n","    ])\n","\n","\n","def cifar10_val_transform(ds_metainfo,\n","                          mean_rgb=(0.4914, 0.4822, 0.4465),\n","                          std_rgb=(0.2023, 0.1994, 0.2010)):\n","    assert (ds_metainfo is not None)\n","    return transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=mean_rgb,\n","            std=std_rgb)\n","    ])"],"metadata":{"id":"L4Gx2PH91Knm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJDP6z9-JoTf"},"outputs":[],"source":["\n","class TransformTwice:\n","    def __init__(self, transform):\n","        self.transform = transform\n","\n","    def __call__(self, input):\n","        output1 = self.transform(input)\n","        output2 = self.transform(input)\n","        return output1, output2\n","\n","\n","class datasets_labeled(datasets.CIFAR10):\n","    def __init__(self, root, indexs=None, train=True,\n","                 transform=None, target_transform=None,\n","                 download=False):\n","        super(datasets_labeled, self).__init__(root, train=train,\n","                 transform=transform, target_transform=target_transform,\n","                 download=download)\n","        if indexs is not None:\n","            self.data = self.data[indexs]\n","            self.targets = np.array(self.targets)[indexs]\n","        self.data = transforms.transpose(transforms.normalize(self.data))\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","\n","        Returns:\n","            tuple: (image, target) where target is index of the target class.\n","        \"\"\"\n","        img, target = self.data[index], self.targets[index]\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return img, target\n","    \n","\n","class datasets_unlabeled(datasets_labeled):\n","\n","    def __init__(self, root, indexs, train=True,\n","                 transform=None, target_transform=None,\n","                 download=False):\n","        super(datasets_unlabeled, self).__init__(root, indexs, train=train,\n","                 transform=transform, target_transform=target_transform,\n","                 download=download)\n","        self.targets = np.array([-1 for i in range(len(self.targets))])\n","\n","\n","def get_datasets(root, n_labeled, datasets,\n","            transform_train=None, transform_val=None,\n","            download=True):\n","\n","            base_dataset = datasets(root, train=True, download=True)\n","            train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(base_dataset.targets, int(n_labeled/10))\n","            train_labeled_dataset = datasets_labeled(root, train_labeled_idxs, train=True, transform=transform_train)\n","            train_unlabeled_dataset = datasets_unlabeled(root, train_unlabeled_idxs, train=True, transform=TransformTwice(transform_train))\n","            val_dataset = datasets_labeled(root, val_idxs, train=True, transform=transform_val, download=True)\n","            test_dataset = datasets_labeled(root, train=False, transform=transform_val, download=True)\n","\n","            print (f\"#Labeled: {len(train_labeled_idxs)} #Unlabeled: {len(train_unlabeled_idxs)} #Val: {len(val_idxs)}\")\n","            return train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset\n","\n","def get_mean_std(root, dataset):\n","    base_dataset = datasets(root, train=True, download=True)\n","    imgs = [item[0] for item in base_dataset] # item[0] and item[1] are image and its label\n","    imgs = torch.stack(imgs, dim=0).numpy()\n","\n","    # calculate mean over each channel (r,g,b)\n","    mean_r = imgs[:,0,:,:].mean()\n","    mean_g = imgs[:,1,:,:].mean()\n","    mean_b = imgs[:,2,:,:].mean()\n","    print(mean_r,mean_g,mean_b)\n","\n","    # calculate std over each channel (r,g,b)\n","    std_r = imgs[:,0,:,:].std()\n","    std_g = imgs[:,1,:,:].std()\n","    std_b = imgs[:,2,:,:].std()\n","    print(std_r,std_g,std_b)\n","\n","    mean = (mean_r, mean_g, mean_b)\n","    std = (std_r, std_g, std_b)\n","\n","    return mean, std\n","\n","\n","def train_val_split(labels, n_labeled_per_class):\n","    labels = np.array(labels)\n","    train_labeled_idxs = []\n","    train_unlabeled_idxs = []\n","    val_idxs = []\n","\n","    for i in range(labels):\n","        idxs = np.where(labels == i)[0]\n","        np.random.shuffle(idxs)\n","        train_labeled_idxs.extend(idxs[:n_labeled_per_class])\n","        train_unlabeled_idxs.extend(idxs[n_labeled_per_class:-500])\n","        val_idxs.extend(idxs[-500:])\n","    np.random.shuffle(train_labeled_idxs)\n","    np.random.shuffle(train_unlabeled_idxs)\n","    np.random.shuffle(val_idxs)\n","\n","    return train_labeled_idxs, train_unlabeled_idxs, val_idxs"]},{"cell_type":"code","source":[],"metadata":{"id":"XNUsOf7AJw4h"},"execution_count":null,"outputs":[]}]}